{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJxYP1AD8fa3",
        "outputId": "484a401e-512c-4ca2-f094-8d45faa76224"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.20)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.29.3)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.15)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.39)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.8.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.4)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.15)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.15)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.15)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.29.3)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (3.4.1)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.1)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (4.49.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (24.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.12.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.14.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (11.1.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain-huggingface) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain-huggingface) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain-huggingface) (0.5.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (3.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.6.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# --- Step-1: Install the Necessary Libraries ---\n",
        "\n",
        "!pip install transformers langchain langchain_community faiss-cpu huggingface_hub pypdf\n",
        "# This command installs several core libraries needed for the RAG (Retrieval Augmented Generation) system:\n",
        "# - `transformers`: Provides pre-trained models and tools for natural language processing (NLP).\n",
        "# - `langchain`: A framework that simplifies the development of applications powered by language models.\n",
        "# - `langchain_community`:  Contains community integrations for LangChain, potentially including specific vector store or LLM providers.\n",
        "# - `faiss-cpu`: A library for efficient similarity search and clustering of dense vectors (used for vector storage and retrieval).\n",
        "# - `huggingface_hub`:  A library that allows you to download and publish models from the Hugging Face Hub.\n",
        "# - `pypdf`: A library to read and work with PDF files.\n",
        "!pip install pymupdf # Install `pymupdf`, a fast and feature-rich PDF and document processing library.\n",
        "!pip install -U langchain # Upgrade `langchain` to the latest version to ensure compatibility with the latest features and fixes.\n",
        "!pip install --upgrade langchain # Upgrade `langchain` again to ensure all dependencies are up-to-date and compatible.\n",
        "!pip install -U langchain langchain-huggingface # Upgrade `langchain` and install `langchain-huggingface`\n",
        "\n",
        "\n",
        "# --- Step-2: Import the Required Modules from the Installed Libraries ---\n",
        "\n",
        "import os  # For interacting with the operating system (e.g., environment variables)\n",
        "import requests  # For making HTTP requests (e.g., to Hugging Face API)\n",
        "from langchain_community.vectorstores import FAISS  # Vector store for similarity search using FAISS\n",
        "from langchain.chains import RetrievalQA  # Chain for question-answering with retrieval-based models\n",
        "from langchain_huggingface import HuggingFaceEndpoint  # Hugging Face API integration for endpoints\n",
        "from langchain.text_splitter import CharacterTextSplitter  # Text splitter based on character count\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Recursive text splitter for hierarchical chunking\n",
        "from langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceHubEmbeddings  # Embedding generation using Hugging Face models\n",
        "from langchain.llms import HuggingFaceEndpoint  # Language model integration via Hugging Face endpoints\n",
        "from typing import List  # For specifying list types in function signatures\n",
        "from langchain.embeddings.base import Embeddings  # Abstract base class for embedding interfaces\n",
        "import textwrap  # For formatting and wrapping long text strings\n",
        "import faiss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step-3: Checking if Langchain is Tnstalled Properly ---\n",
        "\n",
        "!pip show langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEY-zv0oKzIm",
        "outputId": "024ba153-1b84-4b13-f5da-f1a06dd241d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.3.21\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: langchain-core, langchain-text-splitters, langsmith, pydantic, PyYAML, requests, SQLAlchemy\n",
            "Required-by: langchain-community\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step-4: Provide the Unique Knowledge Base of Your Choice ---\n",
        "\n",
        "# The following document is a detailed case study of a ransomware attack in a futuristic cyberpunk setting.\n",
        "# It explores how a detective investigates and resolves a sophisticated cybercrime involving stolen research data.\n",
        "# This knowledge base can be used to answer questions about ransomware attacks, cybersecurity, and digital forensics.\n",
        "\n",
        "# Possible Questions the Paragraph Can Answer:\n",
        "# 1. What type of cyberattack did Detective Y investigate?\n",
        "    # (Expected Response: Ransomware attack)\n",
        "# 2. What was the victim's profession?\n",
        "    # (Expected Response: Robotics engineer)\n",
        "# 3. Where was the remote server located that ultimately led to the perpetrator's arrest?\n",
        "    # (Expected Response: Abandoned industrial sector of city X)\n",
        "\n",
        "\n",
        "# Define the document text\n",
        "document_text = \"\"\"\n",
        "The neon lights of X shimmered, reflecting off the sleek cybernetic implants of its citizens. Detective Y, however, saw little of the city's beauty as he hunched over a holographic display, a frown etched on his face. He was facing a digital enigma: a ransomware attack unlike any he'd encountered before. The victim, a renowned robotics engineer named Z, reported that all his research data, years of work on a groundbreaking AI-powered prosthetic limb, had been encrypted. The perpetrator, a shadowy entity calling themselves The Serpent, demanded an exorbitant ransom in untraceable cryptocurrency. Y, a veteran of the Cyber Crimes Division, knew that time was of the essence. Z's research was not only invaluable scientifically but also held the potential to revolutionize prosthetics for millions. But the initial investigation yielded little. The Serpent had left no digital footprints, employing advanced encryption and anonymization techniques to mask their identity and location. Y, however, was not one to be easily deterred. He understood the power of expanding the knowledge base. He requested and received access to Z's entire digital life – his personal computers, lab servers, cloud storage, even his smart home devices. Y's team, equipped with cutting-edge forensic tools, began their meticulous analysis. They reconstructed deleted files, analyzed network traffic logs, and even delved into the firmware of Z's smart appliances, searching for any hidden data or unusual connections. They expanded their search beyond Z's immediate digital sphere, examining online forums, academic databases, and even dark web marketplaces for any mention of the stolen research or clues about The Serpent's identity. As the team dug deeper, they discovered a seemingly unrelated incident: a minor security breach at a local university's robotics lab a few weeks prior. The breach, initially dismissed as a student prank, involved the theft of a small, experimental AI algorithm. Y's intuition flared. Could this be connected to The Serpent's attack? Further investigation revealed a startling connection. The stolen algorithm, while seemingly insignificant on its own, was a crucial component in Z's research. The Serpent, it seemed, had planned their attack meticulously, acquiring the necessary tools before launching their ransomware scheme. With this expanded knowledge base, Y's team was able to trace The Serpent's digital trail. They uncovered a hidden connection to a remote server located in the abandoned industrial sector of X. A raid on the location led to the arrest of a disgruntled former student of Z's, seeking revenge for a perceived academic slight. The case of The Serpent highlighted the crucial role of expanding the knowledge base in digital forensics. By connecting seemingly disparate pieces of information and exploring every digital avenue, Y and his team were able to bring a cybercriminal to justice and safeguard groundbreaking research that held the promise of a better future.\n",
        "\"\"\"\n",
        "\n",
        "# Print the formatted knowledge base with comments for readability\n",
        "print(\"\\nKnowledge Base (Formatted for Readability):\")\n",
        "KnowlwdgeBase = textwrap.fill(document_text, width=170)  # Wrap text to 165 characters per line for better readability\n",
        "print(\"\\n \\t\", KnowlwdgeBase)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gh4GjZLYm8G_",
        "outputId": "998b1a0a-2a95-412f-b708-47a61ecf3b6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Knowledge Base (Formatted for Readability):\n",
            "\n",
            " \t  The neon lights of X shimmered, reflecting off the sleek cybernetic implants of its citizens. Detective Y, however, saw little of the city's beauty as he hunched over a\n",
            "holographic display, a frown etched on his face. He was facing a digital enigma: a ransomware attack unlike any he'd encountered before. The victim, a renowned robotics\n",
            "engineer named Z, reported that all his research data, years of work on a groundbreaking AI-powered prosthetic limb, had been encrypted. The perpetrator, a shadowy entity\n",
            "calling themselves The Serpent, demanded an exorbitant ransom in untraceable cryptocurrency. Y, a veteran of the Cyber Crimes Division, knew that time was of the essence.\n",
            "Z's research was not only invaluable scientifically but also held the potential to revolutionize prosthetics for millions. But the initial investigation yielded little.\n",
            "The Serpent had left no digital footprints, employing advanced encryption and anonymization techniques to mask their identity and location. Y, however, was not one to be\n",
            "easily deterred. He understood the power of expanding the knowledge base. He requested and received access to Z's entire digital life – his personal computers, lab\n",
            "servers, cloud storage, even his smart home devices. Y's team, equipped with cutting-edge forensic tools, began their meticulous analysis. They reconstructed deleted\n",
            "files, analyzed network traffic logs, and even delved into the firmware of Z's smart appliances, searching for any hidden data or unusual connections. They expanded their\n",
            "search beyond Z's immediate digital sphere, examining online forums, academic databases, and even dark web marketplaces for any mention of the stolen research or clues\n",
            "about The Serpent's identity. As the team dug deeper, they discovered a seemingly unrelated incident: a minor security breach at a local university's robotics lab a few\n",
            "weeks prior. The breach, initially dismissed as a student prank, involved the theft of a small, experimental AI algorithm. Y's intuition flared. Could this be connected\n",
            "to The Serpent's attack? Further investigation revealed a startling connection. The stolen algorithm, while seemingly insignificant on its own, was a crucial component in\n",
            "Z's research. The Serpent, it seemed, had planned their attack meticulously, acquiring the necessary tools before launching their ransomware scheme. With this expanded\n",
            "knowledge base, Y's team was able to trace The Serpent's digital trail. They uncovered a hidden connection to a remote server located in the abandoned industrial sector\n",
            "of X. A raid on the location led to the arrest of a disgruntled former student of Z's, seeking revenge for a perceived academic slight. The case of The Serpent\n",
            "highlighted the crucial role of expanding the knowledge base in digital forensics. By connecting seemingly disparate pieces of information and exploring every digital\n",
            "avenue, Y and his team were able to bring a cybercriminal to justice and safeguard groundbreaking research that held the promise of a better future.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step-5: Split and Visualize the Text in Chunks ---\n",
        "\n",
        "def para2Chunks(paragraph, chunk_size=400, chunk_overlap=25):\n",
        "    \"\"\"\n",
        "    Function to split given knowledge base paragraph into smaller chunks.\n",
        "\n",
        "    Args:\n",
        "        paragraph (str): The input text to be split into chunks.\n",
        "        chunk_size (int): The maximum size of each chunk in characters. Default is 400.\n",
        "        chunk_overlap (int): The number of overlapping characters between consecutive chunks. Default is 25.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of text chunks.\n",
        "    \"\"\"\n",
        "    # Ensure the input is a string\n",
        "    if not isinstance(paragraph, str):\n",
        "        raise TypeError(\"Input must be a string.\")  # Raise an error if the input is not a string\n",
        "\n",
        "    # Define the splitter inside the function\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,  # Maximum size of each chunk in characters\n",
        "        chunk_overlap=chunk_overlap,  # Overlap between chunks to preserve context\n",
        "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # Hierarchy of delimiters for splitting\n",
        "        length_function=len  # Use character count as the metric for chunk length\n",
        "    )\n",
        "\n",
        "    # Split the paragraph into chunks using the defined splitter\n",
        "    chunks = splitter.split_text(paragraph)\n",
        "    return chunks  # Return the list of chunks\n",
        "\n",
        "# Split the document text into chunks\n",
        "print(\"\\nChunks and their lengths (Desired Output Upon Successful Chunking):\")\n",
        "chunks = para2Chunks(document_text)  # Call the function to split the document text into chunks\n",
        "\n",
        "# Print only the first few chunks as examples\n",
        "num_examples = 1  # Number of chunks to display as examples\n",
        "print(f\"\\n Displaying the first {num_examples} chunk(s) as example(s):\")\n",
        "for i, chunk in enumerate(chunks[:num_examples]):  # Iterate over the first `num_examples` chunks\n",
        "    print(f\"\\n\\t Chunk {i+1} (Length: {len(chunk)} characters):\")  # Print the chunk index and its length\n",
        "    print(\"--------------------------------\")\n",
        "    chunk_wrapped = textwrap.fill(chunk, width=165)  # Wrap each chunk to 165 characters per line for readability\n",
        "    print(\"\\t\", chunk_wrapped)  # Print the wrapped chunk with indentation\n",
        "\n",
        "# Summarize the remaining chunks that are not displayed\n",
        "remaining_chunks = len(chunks) - num_examples  # Calculate the number of remaining chunks\n",
        "if remaining_chunks > 0:\n",
        "    print(f\"\\n....... {remaining_chunks} more chunks are available but not displayed here.\")\n",
        "    # Inform the user about the number of additional chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPVDSiTGt1pw",
        "outputId": "d0354a09-99ca-4f6a-c681-8a0332bfc17d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chunks and their lengths (Desired Output Upon Successful Chunking):\n",
            "\n",
            " Displaying the first 1 chunk(s) as example(s):\n",
            "\n",
            "\t Chunk 1 (Length: 304 characters):\n",
            "--------------------------------\n",
            "\t The neon lights of X shimmered, reflecting off the sleek cybernetic implants of its citizens. Detective Y, however, saw little of the city's beauty as he hunched\n",
            "over a holographic display, a frown etched on his face. He was facing a digital enigma: a ransomware attack unlike any he'd encountered before\n",
            "\n",
            "....... 8 more chunks are available but not displayed here.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HuggingFaceCustomEmbeddings(Embeddings):\n",
        "    \"\"\"Custom Embeddings class for Hugging Face API.\n",
        "\n",
        "    This class implements the `Embeddings` interface from LangChain, allowing it to be used\n",
        "    with libraries like FAISS for vector storage and retrieval. It interacts with the Hugging Face\n",
        "    Inference API to generate embeddings for text data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, api_url: str, api_token: str):\n",
        "        \"\"\"\n",
        "        Initialize the HuggingFaceCustomEmbeddings instance.\n",
        "\n",
        "        Args:\n",
        "            api_url (str): The URL of the Hugging Face Inference API endpoint.\n",
        "            api_token (str): The API token required for authenticating requests to the Hugging Face API.\n",
        "        \"\"\"\n",
        "        self.api_url = api_url  # Store the API URL for making requests\n",
        "        self.api_token = api_token  # Store the API token for authentication\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"\n",
        "        Generate embeddings for a list of documents.\n",
        "\n",
        "        This method sends a POST request to the Hugging Face API to generate embeddings for multiple\n",
        "        text inputs. Each input is transformed into a fixed-size vector representation.\n",
        "\n",
        "        Args:\n",
        "            texts (List[str]): A list of text strings to generate embeddings for.\n",
        "\n",
        "        Returns:\n",
        "            List[List[float]]: A list of embeddings, where each embedding is represented as a list of floats.\n",
        "                               Returns an empty list if the API request fails.\n",
        "        \"\"\"\n",
        "        # Set up the headers for the HTTP request, including the authorization token\n",
        "        headers = {\"Authorization\": f\"Bearer {self.api_token}\"}\n",
        "\n",
        "        # Define the payload for the POST request, including the input texts and options\n",
        "        payload = {\n",
        "            \"inputs\": texts,  # The list of texts to generate embeddings for\n",
        "            \"options\": {\"wait_for_model\": True}  # Ensure the model is loaded before generating embeddings\n",
        "        }\n",
        "\n",
        "        # Send the POST request to the Hugging Face API\n",
        "        response = requests.post(self.api_url, headers=headers, json=payload)\n",
        "\n",
        "        # Check if the request was successful (status code 200)\n",
        "        if response.status_code == 200:\n",
        "            # Parse and return the JSON response, which contains the embeddings\n",
        "            return response.json()\n",
        "        else:\n",
        "            # Print an error message if the request failed\n",
        "            print(f\"Request failed with status code {response.status_code}\")\n",
        "            print(f\"Error: {response.text}\")\n",
        "            # Return an empty list to indicate failure\n",
        "            return []\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        \"\"\"\n",
        "        Generate an embedding for a single query.\n",
        "\n",
        "        This method is a convenience wrapper around `embed_documents` to handle a single text input.\n",
        "        It calls `embed_documents` with a single-element list and extracts the first embedding.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text string to generate an embedding for.\n",
        "\n",
        "        Returns:\n",
        "            List[float]: The embedding for the input text, represented as a list of floats.\n",
        "        \"\"\"\n",
        "        # Call `embed_documents` with a single-element list and return the first embedding\n",
        "        return self.embed_documents([text])[0]"
      ],
      "metadata": {
        "id": "HxT3__DA9JdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 6: Setting Up the Hugging Face Inference API Embedding ---\n",
        "\n",
        "HF_API_URL = \"https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/paraphrase-MiniLM-L6-v2\"\n",
        "# Define the URL for the Hugging Face Inference API endpoint. This specific URL is for a sentence transformer model.\n",
        "\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'Enter Your API Here' # Set the Hugging Face API token as an environment variable.\n",
        "# NOTE: It is recommended to set the API token as an environment variable for security reasons, rather than hardcoding it directly in the script.\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {os.getenv('HUGGINGFACEHUB_API_TOKEN')}\"\n",
        "}\n",
        "# Create a headers dictionary to include the authorization token in the API requests.\n",
        "# The os.getenv() function retrieves the API token from the environment variable.\n",
        "\n",
        "# Initialize the custom embedding class\n",
        "# Check if the environment variable is set and handle potential None value\n",
        "api_token = os.getenv('HUGGINGFACEHUB_API_TOKEN') # Get the API token from the environment variable.\n",
        "\n",
        "if api_token is None:\n",
        "    raise ValueError(\"HUGGINGFACEHUB_API_TOKEN is not set in the environment.\")\n",
        "# Check if the API token is set. If not, raise a ValueError to indicate that the token is missing.\n",
        "\n",
        "embedding_function = HuggingFaceCustomEmbeddings(\n",
        "    api_url=HF_API_URL,\n",
        "    api_token=api_token\n",
        "    # Alternatively, you can directly pass the token as a string:\n",
        "    # api_token='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'\n",
        ")\n",
        "# Initialize the custom embedding class, passing the API URL and the API token.\n",
        "# The HuggingFaceCustomEmbeddings class is a custom class that handles communication with the Hugging Face API for generating embeddings.\n",
        "\n",
        "masked_api_token = '*' * (len(api_token) - 4) + api_token[-4:]\n",
        "print(f\"API Token (Masked): {masked_api_token}\") # Print the API token."
      ],
      "metadata": {
        "id": "EQqNnBvB8xTu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec1ef45c-4977-4e75-b5f8-8a5ab9500e93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Token (Masked): *********************************Hlux\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step-7: Generating Embeddings for the Correponding Chunks ---\n",
        "\n",
        "def generate_embeddings(texts, api_url, api_token):\n",
        "    \"\"\"\n",
        "    Generate embeddings for a list of texts using the Hugging Face API.\n",
        "\n",
        "    Args:\n",
        "        texts (List[str]): The list of texts to embed.\n",
        "        api_url (str): The Hugging Face API URL.\n",
        "        api_token (str): The Hugging Face API token.\n",
        "\n",
        "    Returns:\n",
        "        List[List[float]]: A list of embeddings, or None if there's an error.\n",
        "    \"\"\"\n",
        "    headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
        "    payload = {\n",
        "        \"inputs\": texts,\n",
        "        \"options\": {\"wait_for_model\": True}\n",
        "    }\n",
        "    try:\n",
        "        response = requests.post(api_url, headers=headers, json=payload)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        else:\n",
        "            print(f\"Request failed with status code: {response.status_code}\")\n",
        "            print(f\"Error: {response.text}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating embeddings: {e}\")\n",
        "        return None\n",
        "\n",
        "# Generate embeddings for all document chunks using the function\n",
        "embeddings = generate_embeddings(chunks, HF_API_URL, os.getenv('HUGGINGFACEHUB_API_TOKEN'))\n",
        "\n",
        "# Initialize text_embeddings_dict with default values\n",
        "text_embeddings_dict = {\n",
        "    \"texts\": chunks,          # Placeholder for the text chunks\n",
        "    \"embeddings\": embeddings # Placeholder for the corresponding embeddings\n",
        "}\n",
        "\n",
        "if embeddings:\n",
        "    # Update text_embeddings_dict with actual data\n",
        "    text_embeddings_dict = {\n",
        "        \"texts\": chunks,                 # List of text chunks\n",
        "        \"embeddings\": embeddings         # Corresponding list of embeddings\n",
        "    }\n",
        "\n",
        "    # --- Print the first embedding (similar to chunk printing) ---\n",
        "num_embedding_examples = 1  # Number of embeddings to display\n",
        "print(\"\\n Embedding for the Corresponding Chunks:\")\n",
        "print(\"-------------------------\")\n",
        "for i, embedding in enumerate(embeddings[:num_embedding_examples]):\n",
        "     print(f\"\\t Embedding {i+1} (Length: {len(embedding)}):\")  # Length of embedding vector\n",
        "     print(f\"\\t{embedding[:5]}... [remaining elements truncated]\")  # Print first 5 elements\n",
        "\n",
        "# Inform the user about the number of additional chunks\n",
        "remaining_embeds = len(embeddings) - num_embedding_examples  # Calculate the number of remaining embeddings\n",
        "if remaining_embeds > 0:\n",
        "    print(f\"\\n....... {remaining_embeds} more embeddings are available but not displayed here.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaMOX3n14lm0",
        "outputId": "5e10698a-e9ba-4b2a-de5e-60e944b8d744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Embedding for the Corresponding Chunks:\n",
            "-------------------------\n",
            "\t Embedding 1 (Length: 384):\n",
            "\t[-0.3275904655456543, 0.39002105593681335, -0.25993314385414124, -0.2682356536388397, 0.20118968188762665]... [remaining elements truncated]\n",
            "\n",
            "....... 8 more embeddings are available but not displayed here.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step-8: Vectorisation of the Embedded Text ---\n",
        "\n",
        "embeddings = embedding_function.embed_documents(chunks)  # Generate embeddings for all document chunks\n",
        "text_embeddings_dict = {\n",
        "    \"texts\": chunks,  # List of text chunks\n",
        "    \"embeddings\": embeddings  # Corresponding list of embeddings\n",
        "}\n",
        "\n",
        "vectorstore = None  # Initialize vectorstore to None\n",
        "\n",
        "if embeddings:\n",
        "    # Create a FAISS vector store from the text chunks and their embeddings\n",
        "    vectorstore = FAISS.from_embeddings(\n",
        "        text_embeddings=list(zip(text_embeddings_dict[\"texts\"], text_embeddings_dict[\"embeddings\"])),\n",
        "        embedding=embedding_function  # Use the same embedding function for consistency\n",
        "    )\n",
        "else:\n",
        "    print(\"Warning: No embeddings were generated. Retrieval might not function correctly.\") # Handle the case where no embeddings were generated.\n",
        "\n",
        "    # Example: Creating an empty FAISS store (requires an embedding dimension)\n",
        "    embedding_model = HuggingFaceEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L12-v2\"\n",
        "    )\n",
        "    embedding_dimension = len(embedding_model.embed_documents([\"test\"])[0]) # Get embedding dimension\n",
        "    vectorstore = faiss.index_factory(embedding_dimension, \"Flat\") # Create an empty FAISS index\n",
        "    #vectorstore = FAISS.from_texts([\"\"], [*embedding_dimension ], embedding_model) #Alternative\n",
        "\n",
        "# --- Print Stored Vectors and Their Structure (Limited to n vectors) ---\n",
        "n = 1  # Set the number of vectors to display\n",
        "e = 4  # Set the number of elements to display\n",
        "print(\"\\nStored Vectors and Their Structure (First {} Vectors):\".format(n))\n",
        "print(\"------------------------------------------------------\")\n",
        "for i, (text, embedding) in enumerate(zip(text_embeddings_dict[\"texts\"][:n], text_embeddings_dict[\"embeddings\"][:n])):\n",
        "    # Wrap the text chunk for better readability\n",
        "    wrapped_text = textwrap.fill(text, width=170)  # Wrap text to 170 characters per line\n",
        "\n",
        "    # Display the first 10 elements of the embedding vector\n",
        "    truncated_embedding = embedding[:e]  # Keep only the first \"e\" elements\n",
        "\n",
        "    print(f\"\\n \\tText Chunk {i + 1}:\")\n",
        "    print(f\"\\t{wrapped_text}\")  # Print the wrapped text\n",
        "    print(f\"\\tEmbedding for Text Chunk {i + 1} (First {e} Elements): {truncated_embedding}... [remaining elements truncated]\")\n",
        "    print(f\"\\tEmbedding Type: {type(embedding)}\")\n",
        "    print(f\"\\tEmbedding Shape: {len(embedding)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rc56A3eey2Cq",
        "outputId": "c329d93c-39c8-4ea5-93fd-ceabe68bd170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stored Vectors and Their Structure (First 1 Vectors):\n",
            "------------------------------------------------------\n",
            "\n",
            " \tText Chunk 1:\n",
            "\tThe neon lights of X shimmered, reflecting off the sleek cybernetic implants of its citizens. Detective Y, however, saw little of the city's beauty as he hunched over a\n",
            "holographic display, a frown etched on his face. He was facing a digital enigma: a ransomware attack unlike any he'd encountered before\n",
            "\tEmbedding for Text Chunk 1 (First 4 Elements): [-0.3275904655456543, 0.39002105593681335, -0.25993314385414124, -0.2682356536388397]... [remaining elements truncated]\n",
            "\tEmbedding Type: <class 'list'>\n",
            "\tEmbedding Shape: 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step-9: Creating the RetrievalQA Chain for Answering the Questions ---\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=\"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "    task=\"text-generation\",\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "# Create the RetrievalQA chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4}),\n",
        "    chain_type=\"stuff\"\n",
        ")\n",
        "\n",
        "# Print configuration details of the QA chain\n",
        "print(\"\\nQA Chain Configuration:\")\n",
        "print(\"-----------------------\")\n",
        "print(f\"\\n\\tLLM Model: {llm.endpoint_url}\")\n",
        "print(f\"\\tLLM Task: {llm.task}\")\n",
        "print(f\"\\tLLM Temperature: {llm.temperature}\")\n",
        "print(f\"\\tRetriever Search Type: {qa_chain.retriever.search_type}\")\n",
        "print(f\"\\tRetriever Top-K: {qa_chain.retriever.search_kwargs.get('k', 'N/A')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UYRsjlSy2N-",
        "outputId": "4bf82e62-5e27-4f3f-d012-cf2406baed2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "QA Chain Configuration:\n",
            "-----------------------\n",
            "\n",
            "\tLLM Model: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\n",
            "\tLLM Task: text-generation\n",
            "\tLLM Temperature: 0.1\n",
            "\tRetriever Search Type: similarity\n",
            "\tRetriever Top-K: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step-10: Process all chunks to generate all embeddings for all chunks ---\n",
        "def processAllText(chunks):\n",
        "    \"\"\"\n",
        "    Generate embeddings for all text chunks and print each chunk\n",
        "    along with its corresponding embedding.\n",
        "\n",
        "    Args:\n",
        "        chunks (List[str]): A list of text chunks.\n",
        "\n",
        "    Returns:\n",
        "        List[List[float]]: A list of embeddings, or None if there's an error.\n",
        "    \"\"\"\n",
        "    # 1. Get Embeddings for All Chunks\n",
        "    #    - Call the `get_embeddings` function to obtain embeddings for the input chunks.\n",
        "    #    - It's crucial that `get_embeddings`, `HF_API_URL`, and `api_token` are defined\n",
        "    #      elsewhere in your code.\n",
        "    embeddings = generate_embeddings(chunks, api_url=HF_API_URL, api_token=api_token)\n",
        "\n",
        "    # 2. Print Header\n",
        "    #    - Print a separator and a descriptive header to the console.\n",
        "    print(\"\\n----- Chunk-Embedding Pairs -----\")\n",
        "\n",
        "    # 3. Process Embeddings (if available)\n",
        "    #    - Check if the `embeddings` were successfully generated (i.e., the list is not empty or None).\n",
        "    if embeddings:\n",
        "        # 4. Iterate Through Chunks and Embeddings\n",
        "        #    - Use `zip` to iterate through the `chunks` and their corresponding `embeddings` in parallel.\n",
        "        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
        "            # 5. Format Chunk Text\n",
        "            #    - Wrap the `chunk` text for better readability on the console.\n",
        "            wrapped_chunk = textwrap.fill(chunk, width=165)  # Adjust width as needed\n",
        "\n",
        "            # 6. Print Chunk Details\n",
        "            #    - Print the chunk number, its length, and the wrapped text.\n",
        "            print(f\"\\nChunk {i + 1}: (Length: {len(chunk)} characters)\")\n",
        "            print(\"----------------------------------------\")\n",
        "            print(f\"\\tText Chunk:\\n{wrapped_chunk}\")\n",
        "\n",
        "            # 7. Print Embedding Details\n",
        "            #    - Print a truncated version of the embedding (first 10 elements) for brevity.\n",
        "            #    - Print the embedding's data type and its shape (length).\n",
        "            print(f\"\\tEmbedding: {embedding[:5]}... [remaining elements truncated]\")\n",
        "            print(f\"\\tEmbedding Type: {type(embedding)}\")\n",
        "            print(f\"\\tEmbedding Shape: {len(embedding)}\")\n",
        "\n",
        "        # 8. Return Embeddings\n",
        "        #    - Return the `embeddings` for further use if they were generated successfully.\n",
        "        return embeddings\n",
        "    else:\n",
        "        # 9. Handle Embedding Generation Failure\n",
        "        #    - If `embeddings` were not generated, print an error message.\n",
        "        print(\"❌ No embeddings were generated. Check the API response or your data.\")\n",
        "        return None  # Or you might want to return an empty list: `return`\n",
        "\n",
        "# Example Usage\n",
        "#     - This code demonstrates how to call the function.\n",
        "#     - Make sure that 'chunks', 'HF_API_URL', and 'api_token' are defined before calling 'processAllText'.\n",
        "all_embeddings = processAllText(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OboUKJuDAzZi",
        "outputId": "ea9d09a3-2e96-48c4-985e-6223d8ef364a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Chunk-Embedding Pairs -----\n",
            "\n",
            "Chunk 1: (Length: 304 characters)\n",
            "----------------------------------------\n",
            "\tText Chunk:\n",
            "The neon lights of X shimmered, reflecting off the sleek cybernetic implants of its citizens. Detective Y, however, saw little of the city's beauty as he hunched\n",
            "over a holographic display, a frown etched on his face. He was facing a digital enigma: a ransomware attack unlike any he'd encountered before\n",
            "\tEmbedding: [-0.3275904655456543, 0.39002105593681335, -0.25993314385414124, -0.2682356536388397, 0.20118968188762665]... [remaining elements truncated]\n",
            "\tEmbedding Type: <class 'list'>\n",
            "\tEmbedding Shape: 384\n",
            "\n",
            "Chunk 2: (Length: 374 characters)\n",
            "----------------------------------------\n",
            "\tText Chunk:\n",
            ". The victim, a renowned robotics engineer named Z, reported that all his research data, years of work on a groundbreaking AI-powered prosthetic limb, had been\n",
            "encrypted. The perpetrator, a shadowy entity calling themselves The Serpent, demanded an exorbitant ransom in untraceable cryptocurrency. Y, a veteran of the Cyber\n",
            "Crimes Division, knew that time was of the essence\n",
            "\tEmbedding: [-0.4547085762023926, 0.2786608636379242, -0.31591296195983887, -0.4058626890182495, -0.1319478452205658]... [remaining elements truncated]\n",
            "\tEmbedding Type: <class 'list'>\n",
            "\tEmbedding Shape: 384\n",
            "\n",
            "Chunk 3: (Length: 356 characters)\n",
            "----------------------------------------\n",
            "\tText Chunk:\n",
            ". Z's research was not only invaluable scientifically but also held the potential to revolutionize prosthetics for millions. But the initial investigation yielded\n",
            "little. The Serpent had left no digital footprints, employing advanced encryption and anonymization techniques to mask their identity and location. Y, however, was\n",
            "not one to be easily deterred\n",
            "\tEmbedding: [-0.45995110273361206, 0.5183820724487305, -0.2689766585826874, -0.0034376305993646383, 0.09348485618829727]... [remaining elements truncated]\n",
            "\tEmbedding Type: <class 'list'>\n",
            "\tEmbedding Shape: 384\n",
            "\n",
            "Chunk 4: (Length: 286 characters)\n",
            "----------------------------------------\n",
            "\tText Chunk:\n",
            ". He understood the power of expanding the knowledge base. He requested and received access to Z's entire digital life – his personal computers, lab servers, cloud\n",
            "storage, even his smart home devices. Y's team, equipped with cutting-edge forensic tools, began their meticulous analysis\n",
            "\tEmbedding: [-0.35080087184906006, 0.1905931532382965, -0.21113812923431396, -0.31975027918815613, -0.1384599357843399]... [remaining elements truncated]\n",
            "\tEmbedding Type: <class 'list'>\n",
            "\tEmbedding Shape: 384\n",
            "\n",
            "Chunk 5: (Length: 396 characters)\n",
            "----------------------------------------\n",
            "\tText Chunk:\n",
            ". They reconstructed deleted files, analyzed network traffic logs, and even delved into the firmware of Z's smart appliances, searching for any hidden data or\n",
            "unusual connections. They expanded their search beyond Z's immediate digital sphere, examining online forums, academic databases, and even dark web marketplaces for\n",
            "any mention of the stolen research or clues about The Serpent's identity\n",
            "\tEmbedding: [-0.6188660860061646, 0.2924731373786926, -0.1343839317560196, -0.1791265904903412, -0.13634595274925232]... [remaining elements truncated]\n",
            "\tEmbedding Type: <class 'list'>\n",
            "\tEmbedding Shape: 384\n",
            "\n",
            "Chunk 6: (Length: 388 characters)\n",
            "----------------------------------------\n",
            "\tText Chunk:\n",
            ". As the team dug deeper, they discovered a seemingly unrelated incident: a minor security breach at a local university's robotics lab a few weeks prior. The breach,\n",
            "initially dismissed as a student prank, involved the theft of a small, experimental AI algorithm. Y's intuition flared. Could this be connected to The Serpent's\n",
            "attack? Further investigation revealed a startling connection\n",
            "\tEmbedding: [-0.19252127408981323, 0.006219435017555952, 0.17461875081062317, -0.3120732605457306, 0.09442088007926941]... [remaining elements truncated]\n",
            "\tEmbedding Type: <class 'list'>\n",
            "\tEmbedding Shape: 384\n",
            "\n",
            "Chunk 7: (Length: 331 characters)\n",
            "----------------------------------------\n",
            "\tText Chunk:\n",
            ". The stolen algorithm, while seemingly insignificant on its own, was a crucial component in Z's research. The Serpent, it seemed, had planned their attack\n",
            "meticulously, acquiring the necessary tools before launching their ransomware scheme. With this expanded knowledge base, Y's team was able to trace The Serpent's\n",
            "digital trail\n",
            "\tEmbedding: [-0.5182772278785706, 0.28702089190483093, -0.2649579346179962, -0.1738174557685852, 0.11443278938531876]... [remaining elements truncated]\n",
            "\tEmbedding Type: <class 'list'>\n",
            "\tEmbedding Shape: 384\n",
            "\n",
            "Chunk 8: (Length: 340 characters)\n",
            "----------------------------------------\n",
            "\tText Chunk:\n",
            ". They uncovered a hidden connection to a remote server located in the abandoned industrial sector of X. A raid on the location led to the arrest of a disgruntled\n",
            "former student of Z's, seeking revenge for a perceived academic slight. The case of The Serpent highlighted the crucial role of expanding the knowledge base in\n",
            "digital forensics\n",
            "\tEmbedding: [-0.39854755997657776, 0.2911760210990906, 0.056889306753873825, -0.19699566066265106, 0.06826524436473846]... [remaining elements truncated]\n",
            "\tEmbedding Type: <class 'list'>\n",
            "\tEmbedding Shape: 384\n",
            "\n",
            "Chunk 9: (Length: 234 characters)\n",
            "----------------------------------------\n",
            "\tText Chunk:\n",
            ". By connecting seemingly disparate pieces of information and exploring every digital avenue, Y and his team were able to bring a cybercriminal to justice and\n",
            "safeguard groundbreaking research that held the promise of a better future.\n",
            "\tEmbedding: [-0.21212005615234375, 0.24919500946998596, -0.34464603662490845, -0.507266640663147, 0.06745190918445587]... [remaining elements truncated]\n",
            "\tEmbedding Type: <class 'list'>\n",
            "\tEmbedding Shape: 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step-11: Establishing a Chat Interface ---\n",
        "\n",
        "# Print a welcome message indicating that the RAG model is ready for interaction\n",
        "print(\"\\n||🤖 Your RAG Model is Ready! Type 'exit' to quit.||\\n\")\n",
        "\n",
        "# Start an infinite loop to continuously accept user input\n",
        "while True:\n",
        "    # Prompt the user to enter their question\n",
        "    query = input(\"🟢 Your Question: \")\n",
        "\n",
        "    # Check if the user wants to exit the chat interface\n",
        "    if query.lower() == \"exit\":\n",
        "        print(\"\\n👋 Goodbye!\")  # Print a goodbye message\n",
        "        break  # Exit the loop and terminate the program\n",
        "\n",
        "    try:\n",
        "        # Use the RetrievalQA chain (`qa_chain`) to process the user's query\n",
        "        response = qa_chain.invoke(query)\n",
        "\n",
        "        # Wrap the response text for better readability\n",
        "        wrapped_response = textwrap.fill(response['result'], width=160)  # Adjust width as needed\n",
        "\n",
        "        # Print the wrapped response from the RAG model\n",
        "        print(f\"\\n🔵 RAG's Answer:{wrapped_response}\\n\")\n",
        "    except Exception as e:\n",
        "        # Handle any errors that occur during the query processing\n",
        "        print(f\"\\n❌ **Error:** {e}\")  # Print the error message for debugging\n",
        "else:\n",
        "    # This block is executed if the loop completes without a `break` statement\n",
        "    # However, since the loop is infinite, this block will never be reached unless modified\n",
        "    print(\"❌ No embeddings were generated. Please check the error in API response.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l44spAS_NvGB",
        "outputId": "9cf10351-898c-4b38-cb89-6162a63fa972"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "||🤖 Your RAG Model is Ready! Type 'exit' to quit.||\n",
            "\n",
            "🟢 Your Question: What type of cyberattack did Detective Y investigate? \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔵 RAG's Answer: Detective Y investigated a ransomware attack.\n",
            "\n",
            "🟢 Your Question: exit\n",
            "\n",
            "👋 Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TEST QUESTIONS:**\n",
        "\n",
        "## In-Text Questions:\n",
        "\n",
        "1. What type of cyberattack did Detective Y investigate? (Expected Response: Ransomware attack)\n",
        "2. What was the victim's profession? (Expected Response: Robotics engineer)\n",
        "3. Where was the remote server located that ultimately led to the perpetrator's arrest? (Expected Response: Abandoned industrial sector of city X)\n",
        "\n",
        "\n",
        "## Out-of-Text Questions:\n",
        "1. What specific encryption algorithm did The Serpent use to encrypt the research data? (Expected Response: The story doesn't mention the specific algorithm.)\n",
        "2. What was the name of the university where the minor security breach occurred? (Expected Response: The story doesn't mention the university's name.)\n",
        "3. Did Detective Y's team collaborate with any external cybersecurity experts or organizations during the investigation? (Expected Response: The story doesn't mention any external collaboration.)"
      ],
      "metadata": {
        "id": "49HZsr_4PewJ"
      }
    }
  ]
}